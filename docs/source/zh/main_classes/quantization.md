<!--ç‰ˆæƒ2023å¹´HuggingFaceå›¢é˜Ÿã€‚ç‰ˆæƒæ‰€æœ‰ã€‚

æ ¹æ®Apacheè®¸å¯è¯2.0ç‰ˆæœ¬ï¼ˆâ€œè®¸å¯è¯â€ï¼‰ï¼Œæ‚¨é™¤éç¬¦åˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨
è®¸å¯è¯ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯çš„å‰¯æœ¬

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œå¦åˆ™æ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æ˜¯æŒ‰â€œç°çŠ¶â€åˆ†å‘çš„ï¼Œæ²¡æœ‰ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯çš„
ç‰¹å®šè¯­è¨€ç®¡ç†æƒé™å’Œé™åˆ¶æ¡ä»¶çš„é™åˆ¶ã€‚

âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶æ˜¯Markdownæ ¼å¼çš„ï¼Œä½†åŒ…å«æˆ‘ä»¬doc-builderçš„ç‰¹å®šè¯­æ³•ï¼ˆç±»ä¼¼äºMDXï¼‰ï¼Œåœ¨æ‚¨çš„MarkdownæŸ¥çœ‹å™¨ä¸­å¯èƒ½æ— æ³•æ­£ç¡®å‘ˆç°ã€‚

-->

# é‡åŒ– ğŸ¤— Transformers æ¨¡å‹

## `bitsandbytes` é›†æˆ

ğŸ¤— Transformers ä¸ `bitsandbytes` ä¸Šä½¿ç”¨æœ€å¤šçš„æ¨¡å—å¯†åˆ‡é›†æˆã€‚æ‚¨å¯ä»¥åœ¨å‡ è¡Œä»£ç ä¸­ä»¥8ä½ç²¾åº¦åŠ è½½æ¨¡å‹ã€‚
ä»`bitsandbytes`å‘å¸ƒçš„`0.37.0`ç‰ˆæœ¬å¼€å§‹ï¼Œå¤§å¤šæ•°GPUç¡¬ä»¶éƒ½æ”¯æŒæ­¤åŠŸèƒ½ã€‚

æœ‰å…³é‡åŒ–æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[LLM.int8()](https://arxiv.org/abs/2208.07339)è®ºæ–‡ï¼Œæˆ–è€…å…³äºåˆä½œçš„[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ã€‚
ä»`0.39.0`ç‰ˆæœ¬å¼€å§‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨4ä½é‡åŒ–åŠ è½½æ”¯æŒ`device_map`çš„ä»»ä½•æ¨¡å‹ï¼Œåˆ©ç”¨FP4æ•°æ®ç±»å‹ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨`bitsandbytes`é›†æˆå¯ä»¥æ‰§è¡Œçš„æ“ä½œ

### FP4é‡åŒ– 

#### è¦æ±‚

åœ¨è¿è¡Œä¸‹é¢çš„ä»£ç ç‰‡æ®µä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹è¦æ±‚ã€‚

- æœ€æ–°çš„`bitsandbytes`åº“
`pip install bitsandbytes>=0.39.0`

- ä»æºä»£ç å®‰è£…æœ€æ–°çš„`accelerate`
`pip install git+https://github.com/huggingface/accelerate.git`

- ä»æºä»£ç å®‰è£…æœ€æ–°çš„`transformers` 
`pip install git+https://github.com/huggingface/transformers.git`

#### ä½¿ç”¨4ä½åŠ è½½å¤§å‹æ¨¡å‹

åœ¨è°ƒç”¨`.from_pretrained`æ–¹æ³•æ—¶ä½¿ç”¨`load_in_4bit=True`ï¼Œå¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘4å€ï¼ˆå¤§è‡´ï¼‰ã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_4bit=True)
```

<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦æ¨¡å‹ä»¥4ä½åŠ è½½ï¼Œç›®å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubã€‚è¯·æ³¨æ„ï¼Œå°šä¸æ”¯æŒè®­ç»ƒ4ä½æƒé‡ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨4ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚

</Tip>

### ä½¿ç”¨8ä½åŠ è½½å¤§å‹æ¨¡å‹

é€šè¿‡åœ¨è°ƒç”¨`.from_pretrained`æ–¹æ³•æ—¶ä½¿ç”¨`load_in_8bit=True`å‚æ•°ï¼Œå¯ä»¥å°†å†…å­˜è¦æ±‚å¤§è‡´å‡åŠã€‚


```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", load_in_8bit=True)
```

ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨[`PreTrainedModel`]ä¸€æ ·ä½¿ç”¨æ¨¡å‹ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨`get_memory_footprint`æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚

```python
print(model.get_memory_footprint())
```

é€šè¿‡æ­¤é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§å‹æ¨¡å‹å¹¶æ— ç¼è¿è¡Œå®ƒä»¬ã€‚

<Tip warning={true}>

è¯·æ³¨æ„ï¼Œä¸€æ—¦æ¨¡å‹ä»¥8ä½åŠ è½½ï¼Œç›®å‰æ— æ³•å°†é‡åŒ–çš„æƒé‡æ¨é€åˆ°Hubï¼Œé™¤éæ‚¨ä½¿ç”¨æœ€æ–°çš„`transformers`å’Œ`bitsandbytes`ã€‚è¯·æ³¨æ„ï¼Œå°šä¸æ”¯æŒè®­ç»ƒ8ä½æƒé‡ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨8ä½æ¨¡å‹æ¥è®­ç»ƒé¢å¤–çš„å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä»‹ç»ã€‚
è¿˜è¦æ³¨æ„`device_map`æ˜¯å¯é€‰çš„ï¼Œä½†ä¸ºäº†æ¨ç†æ•ˆæœå¥½ï¼Œè®¾ç½®`device_map = 'auto'`æ˜¯é¦–é€‰ï¼Œå®ƒå°†æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†æ´¾åˆ°å¯ç”¨çš„èµ„æºä¸Šã€‚

</Tip>

#### é«˜çº§ç”¨ä¾‹

è¿™é‡Œå°†ä»‹ç»ä¸€äº›ä½¿ç”¨FP4é‡åŒ–çš„é«˜çº§ç”¨ä¾‹

##### æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹

è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ›´æ”¹è®¡ç®—è¿‡ç¨‹ä¸­å°†ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œéšè—çŠ¶æ€å¯ä»¥ä¸º`float32`ï¼Œä½†è®¡ç®—å¯ä»¥è®¾ç½®ä¸ºbf16ä»¥æé«˜é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è®¾ç½®ä¸º`float32`ã€‚

```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

##### ä½¿ç”¨NF4ï¼ˆNormal Float 4ï¼‰æ•°æ®ç±»å‹ 

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨NF4æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡çš„æ–°çš„4ä½æ•°æ®ç±»å‹ã€‚ä¸ºæ­¤è¿è¡Œ:

```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

##### ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨æ–­

æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚è¿™æ ·å¯ä»¥èŠ‚çœæ›´å¤šçš„å†…å­˜ï¼Œè€Œä¸ä¼šå¢åŠ é¢å¤–çš„æ€§èƒ½-æ ¹æ®æˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿï¼Œè¿™ä½¿å¾—åœ¨å…·æœ‰1024ä¸ªåºåˆ—é•¿åº¦ã€1ä¸ªæ‰¹æ¬¡å¤§å°å’Œ4ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„NVIDIA-T4 16GBä¸Šå¾®è°ƒllama-13bæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

```python
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)
```


### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub

æ‚¨å¯ä»¥é€šè¿‡ç®€å•åœ°ä½¿ç”¨`push_to_hub`æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°Hubã€‚è¿™é¦–å…ˆä¼šæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚
è¯·ç¡®ä¿ä½¿ç”¨`bitsandbytes>0.37.2`ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬åœ¨`bitsandbytes==0.38.0.post1`ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼‰ä»¥ä¾¿ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚ 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

<Tip warning={true}>

å¼ºçƒˆå»ºè®®å°†8ä½æ¨¡å‹æ¨é€åˆ°Hubä»¥ç”¨äºå¤§å‹æ¨¡å‹ã€‚è¿™å°†ä½¿ç¤¾åŒºèƒ½å¤Ÿä»å†…å­˜å ç”¨å‡å°‘å’ŒåŠ è½½çš„å¥½å¤„ä¸­å—ç›Šï¼Œä¾‹å¦‚åœ¨Google Colabä¸ŠåŠ è½½å¤§å‹æ¨¡å‹ã€‚

</Tip>

### ä»ğŸ¤— HubåŠ è½½é‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨å±æ€§`quantization_config`ï¼Œç¡®ä¿æ¨é€çš„æƒé‡ç»è¿‡é‡åŒ–ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æŒ‡å®šå‚æ•°`load_in_8bit=True`ï¼Œä½†æ‚¨éœ€è¦ç¡®ä¿å·²å®‰è£…`bitsandbytes`å’Œ`accelerate`ã€‚
è¿˜è¦æ³¨æ„`device_map`æ˜¯å¯é€‰çš„ï¼Œä½†ä¸ºäº†æ¨ç†æ•ˆæœå¥½ï¼Œè®¾ç½®`device_map = 'auto'`æ˜¯é¦–é€‰ï¼Œå®ƒå°†æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†æ´¾åˆ°å¯ç”¨çš„èµ„æºä¸Šã€‚

### é«˜çº§ç”¨ä¾‹

æœ¬èŠ‚é¢å‘é«˜çº§ç”¨æˆ·ï¼Œå¸Œæœ›æ¢ç´¢åœ¨åŠ è½½å’Œè¿è¡Œ8ä½æ¨¡å‹ä¹‹å¤–å¯ä»¥åšä»€ä¹ˆã€‚

#### åœ¨`cpu`å’Œ`gpu`ä¹‹é—´è¿›è¡Œå¸è½½

è¿™å…¶ä¸­ä¸€ä¸ªé«˜çº§ç”¨ä¾‹æ˜¯èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶åœ¨`CPU`å’Œ`GPU`ä¹‹é—´åˆ†æ´¾æƒé‡ã€‚è¯·æ³¨æ„ï¼Œå°†åœ¨CPUä¸Šåˆ†æ´¾çš„æƒé‡**ä¸ä¼š**è½¬æ¢ä¸º8ä½ï¼Œå› æ­¤ä¿æŒä¸º`float32`ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºå¸Œæœ›é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶åœ¨GPUå’ŒCPUä¹‹é—´åˆ†æ´¾æ¨¡å‹çš„ç”¨æˆ·ã€‚

é¦–å…ˆï¼Œä»`transformers`åŠ è½½`BitsAndBytesConfig`å¹¶å°†å±æ€§`llm_int8_enable_fp32_cpu_offload`è®¾ç½®ä¸º`True`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

å‡è®¾æ‚¨æƒ³è¦åŠ è½½`bigscience/bloom-1b7`æ¨¡å‹ï¼Œå¹¶ä¸”æ‚¨çš„GPU RAMåˆšå¥½è¶³å¤Ÿå®¹çº³æ•´ä¸ªæ¨¡å‹ï¼Œé™¤äº†`lm_head`ã€‚å› æ­¤ï¼Œç¼–å†™è‡ªå®šä¹‰çš„device_mapå¦‚ä¸‹:
```python
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

å¹¶æŒ‰ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹:
```python
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

å°±æ˜¯è¿™æ ·ï¼äº«å—æ‚¨çš„æ¨¡å‹å§ï¼

#### ä½¿ç”¨`llm_int8_threshold`è¿›è¡Œè°ƒæ•´

æ‚¨å¯ä»¥ä½¿ç”¨`llm_int8_threshold`å‚æ•°æ¥æ›´æ”¹å¼‚å¸¸å€¼çš„é˜ˆå€¼ã€‚"å¼‚å¸¸å€¼"æ˜¯å¤§äºæŸä¸ªç‰¹å®šé˜ˆå€¼çš„éšè—çŠ¶æ€å€¼ã€‚
è¿™å¯¹åº”äº`LLM.int8()`è®ºæ–‡ä¸­æè¿°çš„å¼‚å¸¸å€¼æ£€æµ‹çš„å¼‚å¸¸å€¼é˜ˆå€¼ã€‚ä»»ä½•è¶…è¿‡æ­¤é˜ˆå€¼çš„éšè—çŠ¶æ€å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¹¶ä¸”åœ¨è¿™äº›å€¼ä¸Šçš„æ“ä½œå°†ä»¥fp16è¿›è¡Œã€‚è¿™äº›å€¼é€šå¸¸æœä»æ­£æ€åˆ†å¸ƒï¼Œå³å¤§å¤šæ•°å€¼åœ¨[-3.5, 3.5]èŒƒå›´å†…ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹ï¼Œæœ‰ä¸€äº›ä¾‹å¤–çš„ç³»ç»Ÿå¼‚å¸¸å€¼åˆ†å¸ƒéå¸¸ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´[-60, -6]æˆ–[6, 60]å†…ã€‚å¯¹äºå¤§å°çº¦ä¸º5çš„å€¼ï¼Œint8é‡åŒ–æ•ˆæœå¾ˆå¥½ï¼Œä½†è¶…è¿‡è¿™ä¸ªèŒƒå›´ï¼Œæ€§èƒ½ä¼šæ˜æ˜¾ä¸‹é™ã€‚ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°å‹æ¨¡å‹ï¼Œå¾®è°ƒï¼‰ï¼Œå¯èƒ½éœ€è¦è¾ƒä½çš„é˜ˆå€¼ã€‚
æ­¤å‚æ•°ä¼šå½±å“æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å»ºè®®å°è¯•ä¸åŒçš„å‚æ•°å€¼ï¼Œæ‰¾åˆ°æœ€é€‚åˆæ‚¨ç”¨ä¾‹çš„å‚æ•°å€¼ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢

æŸäº›æ¨¡å‹å…·æœ‰å¤šä¸ªæ¨¡å—ï¼Œè¿™äº›æ¨¡å—åœ¨é‡åŒ–æ—¶ä¸åº”è½¬æ¢ä¸º8ä½ï¼Œä»¥ç¡®ä¿ç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼ŒJukeboxæ¨¡å‹å…·æœ‰å¤šä¸ªåº”è·³è¿‡çš„`lm_head`æ¨¡å—ã€‚ä½¿ç”¨`llm_int8_skip_modules`è¿›è¡Œå°è¯• 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### åœ¨8ä½åŠ è½½çš„æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒ

åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­æ”¯æŒé€‚é…å™¨çš„å®˜æ–¹æ”¯æŒä¸‹ï¼Œæ‚¨å¯ä»¥å¾®è°ƒå·²ç»ä»¥8ä½åŠ è½½çš„æ¨¡å‹ã€‚
è¿™ä½¿å¾—å¯ä»¥åœ¨å•ä¸ªGoogle Colabä¸­å¾®è°ƒè¯¸å¦‚`flan-t5-large`æˆ–`facebook/opt-6.7b`ä¹‹ç±»çš„å¤§å‹æ¨¡å‹ã€‚è¯·å‚é˜…[`peft`](https://github.com/huggingface/peft)åº“ä»¥äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

è¯·æ³¨æ„ï¼Œåœ¨åŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶ä¸éœ€è¦ä¼ é€’`device_map`å‚æ•°ã€‚å®ƒå°†è‡ªåŠ¨å°†æ¨¡å‹åŠ è½½åˆ°æ‚¨çš„GPUä¸Šã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨è¿˜å¯ä»¥å°†è®¾å¤‡æ˜ å°„è®¾ç½®ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚`cuda:0`ï¼Œ`0`ï¼Œ`torch.device('cuda:0')`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œä»…æ¨ç†æ—¶åº”ä½¿ç”¨`device_map=auto`ã€‚ 

### BitsAndBytesConfig

[[autodoc]] BitsAndBytesConfig


## ä½¿ç”¨ğŸ¤— `optimum` è¿›è¡Œé‡åŒ– 

è¯·æŸ¥çœ‹[Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/index)ä»¥äº†è§£æœ‰å…³`optimum`æ”¯æŒçš„é‡åŒ–æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ã€‚

